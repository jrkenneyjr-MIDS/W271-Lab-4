---
title: "Lab 4"
author: "Ram Balasubramanian, John Kenney"
date: "November 28, 2017"
output: pdf_document
---

```{r include = FALSE}
library(knitr)
opts_chunk$set(tidy.opts=list(width.cutoff=60),tidy=TRUE)

library(xts)
library(tseries)
library(lubridate)
library(forecast)
```

#Description of the Lab

In this lab, you are provided a time series stored in "Lab4-series2.csv". The series, which is a monthly series,
covers the historical period from 1990 to 2015. Your main task is to build a time series model, using materials
covered between lecture 6 - lecture 10 (not including VAR modeling), and conduct a monthly, 11-month
ahead forecast of the series in 2015.

You will have to split the series into a training series, which includes the data from 1990 to 2014 December,
leaving all the months in 2015 as the test data.

As we have studied extensively in the last 6 lectures on how to build time series models, I expect that
your team illustrate and describe all of steps in building up to your final model. All of your analyses and
decisions to select your final models must be clearly demonstrated. It is quite possible that you will have
a few candidate models that you have considered. Besides using information criterion, consider the mean
absolute percentage error (MAPE) on the test sample in your model selection.

```{r}
x = as.data.frame(read.csv("Lab4-series2.csv", header = TRUE))
colnames(x) <- c("num", "value")
head(x)
```



```{r}
idx <- seq(as.Date("1990/1/1"), by = "month", length.out = dim(x)[1])

#Split Timeseries into Train and Test sets
ts_all <- xts(x['value'], order.by=idx)
str(ts_all)
head(ts_all,10)
options(digits = 4)
#Create train set from 1990-2014
ts.train = ts_all['1990-01-01/2014-12-31']
ts.test = ts_all["2015-01-01/"]
```
# Exploratory Data Analysis:

## Plot the time series
```{r}
plot.xts(ts.train, type='l', minor.ticks = NULL, major.ticks = 12, main="Time Series Plot", grid.ticks.on="years")
```
## Add smoother to see if there is visual indication of a trend or seasonality
```{r include=FALSE}
#Add Smoothing
library(fpp)
k.smooth.wide <- ksmooth(time(ts.train), ts.train, kernel = c("normal"), bandwidth = 200)
ksw = as.xts(k.smooth.wide$y, order.by = index(ts.train))
k.smooth.narrow <- ksmooth(time(ts.train), ts.train, kernel = c("normal"), bandwidth = 100)
ksn = as.xts(k.smooth.narrow$y, order.by = index(ts.train))

hw = HoltWinters(ts.train, alpha = 0.1, beta = 1, gamma = F)
plot(hw)
hw.xhat = hw$fitted[,'xhat']
hw.xts = xts(hw.xhat, order.by = index(ts.train[3:300]))
tw = cbind(ts.train[3:300], ksw[3:300], ksn[3:300], hw.xts)
colnames(tw) = c("Train", "Wide", "Narrow", "HW")

plot.xts(tw, type='l', col = c('black', 'blue', "green"), minor.ticks = NULL, 
         major.ticks = 12, main="Time Series Plot", grid.ticks.on="years",
         xlim = c(min(index(ts.train)), max(index(ts.train))))
```

##  Fit a Linear Model to see if statistically significant trend
```{r}
trnd = lm(x, formula = value ~ num)
summary(trnd)
```
## Observations:  
Trend:  Series doesn't show any overall trend;  there appear to be periods of "shock" where the series goes up quickly and slowly dissipates from those highs. In general the series exhibits "random walk" behavior.  
Seasonal & Cyclical patterns:  There are some seasonal patterns apparent in the series. There is also a broad cyclical pattern (ups and downs) - but the periodicity doesn't appear to be consistent.   
Outliers:  There aren't any apparent outliers.



We see a statistically significant trend, albeit nearly zero.


# Check for Seasonality 
```{r}
#decompose(ts)
#Decompose the series for seasonality, trends, etc.
#STL: Seasonal decomposition of Timeseries by Loess
#stl(ts, s.window = "periodic", s.degree = 1, t.degree = 1, robust = F)

```
Seasonal decomposition using decompose() and stl() show no evidence of seasonality.

#Use the Holt Winters Method to Confirm


## Check for AR 1 Random Walk
```{r}
#is the data stationary in the mean/variance?
#Let's look at the acf and pacf plots
  par(mfrow=c(1,2))
  acf(ts.train, main="ACF of Given Series")
  pacf(ts.train, main="PACF of Given Series")
```
## Observations:  
The ACF declines very slowly (indicative of an AR(1) process).  
The PACF shows significance at lag-1.  There is also significant correlation at lags 4,7,10,11,13. Note that correlation at lags 7 and 13 is negative. 



```{r echo=FALSE}

#Conduct Ljung-Box test to see if data are serially correlated
  BoxPValue = Box.test(ts.train,type='Ljung-Box',lag=20)$p.value
  result = "Data are Random"
  if (BoxPValue < 0.05){ result = "Data are NOT random "} 
  print(paste(result, " - p-value of box-test = ",BoxPValue))

#conduct Dicky-Fuller test to see if series is stationary
  adfPvalue = adf.test(ts.train)$p.value
  result = "Unit Root Present - Non Stationary Series"
  if (adfPvalue < 0.05){ result = " Reject Unit Root Presence - Stationary Series"} 
  print(paste(result, " - p-value of adf-test =",adfPvalue))

monthplot(ts.train)
```
The Ljung-Box test indicates there is significant auto-correlation over 20 lags.  This confirms the slow decay of correlation over lags in the ACF.  

Dickey-Fuller test indicates train set is not stationary, since we cannot reject the null hypothesis that there is a unit root.


## Dicky-Fuller test shows that the series is not stationary.  While there is no clear trend in the data - it does exhibit random-walk behavior.  Let's apply a first difference to see if that makes the series stationary.



## First Difference
```{r}
diff1 = diff(ts.train, lag=1, differences=1, na.pad = F)
autoplot.zoo(diff1)
#hwd = HoltWinters(diff1, alpha = 0.2, beta, 0.2, gamma = 1, seasonal = "additive")

acf(diff1, na.action=na.pass)
pacf(diff1, na.action=na.pass)

adfPvalue = adf.test(diff1)$p.value

result = "Unit Root Present - Non Stationary Series"
if (adfPvalue < 0.05){ result = " Reject Unit Root Presence - Stationary Series"} 
print(paste(result, " - p-value of adf-test =",adfPvalue))

#JK 11/30
mean(ts)
mean(diff1)

#Decompose, stl shows no seasonality
#decompose(diff1)

#Plot residuals, add a smoother

#Add linear regression on month, ANOVA by month, visualization
#methods(forecast)
#Holt-Winters, STL, 
```



```{r}
#Percentage Error: p_i=100e_i/y_i
#MAPE: MAPE=mean(|p_i|).
MAPE = function(y.meas, y.pred){
  m = c()
  for (i in seq(1, length(y.meas))){
    m[i] = abs((y.meas[i]-y.pred[i])/y.meas[i])
  }
  return(mean(m))
}

#sMAPE: sMAPE=mean(200|y_i???y_i_hat|/(y_i+y_i_hat))
#Cite: https://robjhyndman.com/hyndsight/smape/
sMAPE = function(y.meas, y.pred){
  s = c()
  for (i in seq(1, length(y.meas))){
    s[i] = 2*abs(y.meas[i]-y.pred[i])/(abs(y.meas[i])+abs(y.pred[i]))
  }
  return(mean(s))
}

#Add additional qualification methods (MAE, MPE, AC1, Thiel's U, etc.)
arima.loop = function(p,d,q,P,D,Q,M){
  df = data.frame(matrix(vector(), nrow = 0, ncol =  11,
                dimnames=list(c(),
                c("p",'d','q','P','D','Q','m',"AIC","BIC","MAPE","sMAPE"))),
                stringsAsFactors=F)
  
  for (m in M){
    for (D in seq(0,D)){
      for (P in seq(0,P)){
        for (Q in seq(0,Q)){
          for (d in seq(0,d)){
            for (p in seq(0,p)){
              for (q in seq(0,q)){
               tryCatch(
                  {
                   modfit = arima(ts, order = c(p,d,q), 
                      seasonal = list(order = c(P,D,Q),period=m),
                      method = "ML")
                    mf.aic = modfit$aic
                    mf.bic = BIC(modfit)
                    pr = predict(modfit, n.ahead = 11)
                    mf.mape = MAPE(ts.test, pr$pred)
                    mf.smape = sMAPE(ts.test, pr$pred)
                    df[dim(df)[1]+1,] = c(p,d,q,P,D,Q,m,mf.aic,mf.bic,mf.mape,mf.smape)
                  },
                  error=function(e){
                    df[dim(df)[1]+1,] = c(p,d,q,P,D,Q,m,NA,NA,NA,NA)
                  },
                  warning=function(cond){
                    return(NULL)
                  }
                )
                
              }}}}}}}
  return(df)
  }


test.arloop = arima.loop(2,1,1,1,1,1,c(12))

#Which model is has lowest MAPE?
test.arloop[order(test.arloop$MAPE), ]

#Which model is has lowest symmetric MAPE?
head(test.arloop[order(test.arloop$sMAPE), ], 10)

#Which model has lowest AIC?  
  #Should we also include corrected AIC?
  #Probably not.  All models will have same correction factor because they are all the same length.
head(test.arloop[order(test.arloop$AIC), ], 10)
```

```{r}


#Is this the best model to predict 2016?  
#Probably more robust estimation is to use cross-validation
#Cross-Validation of Time Series: https://robjhyndman.com/hyndsight/tscv/
#A.K.A. Evaluation on a Rolling Forecasting Origin  


#Can we use multiple seasons (e.g. 12 and 4?)
best.arima = arima(ts.train, order = c(2,1,1),
                   seasonal = list(order = c(1,1,0),period=6),
                   method = "ML")

acf(best.arima$residuals)
pacf(best.arima$residuals)
tsdiag(best.arima)
best.arima$aic
BIC(best.arima)
pr = predict(best.arima, n.ahead = 11)
sMAPE(ts.test,pr$pred)
MAPE(ts.test,pr$pred)

best.arima2 = arima(ts.train, order = c(2,0,0),
                   seasonal = list(order = c(0,1,0),period=12),
                   method = "ML")
acf(best.arima2$residuals)
pacf(best.arima2$residuals)
tsdiag(best.arima2)

best.arima2$aic
BIC(best.arima2)
pr = predict(best.arima2, n.ahead = 11)
sMAPE(ts.test,pr$pred)
MAPE(ts.test,pr$pred)

best.arima3 = arima(ts.train, order = c(2,0,0),
                   seasonal = list(order = c(1,1,1),period=12),
                   method = "ML")
acf(best.arima2$residuals)
pacf(best.arima2$residuals)
tsdiag(best.arima2)

best.arima3$aic
BIC(best.arima3)
pr = predict(best.arima3, n.ahead = 11)
sMAPE(ts.test,pr$pred)
MAPE(ts.test,pr$pred)

#Try loops with the other ARIMA methods (ML, CSS)

```
Observations:
Taking the first difference makes the series stationary.  But the plot of the series and the ACF shows a strong seasonal pattern. The ACF plot shows strong  correlations at lag 3 (negative), 6 (postive), 9 (-ve), 12 (+ve).  The PACF shows similar seasonal patterns. 
Let's investigate that further. 

JK 11/30
Notice that significant trends shifted from 13 to 12. The differenced series also has no trend when using the decompose() function.  We know that the mean of the differenced series is practically zero, so there is no trend we have to address if we go this route.

## Investigate seasonality
```{r}
#Look at monthly averages
plot(aggregate(ts.train, month(index(ts.train)), mean), type='o')
monthplot(ts.train)
```
### The month plot shows there are "high seasons" and "low seasons". 

JK 11/30
What's the variance look like?  Are these differences significant?

```{r}
#since this is a monthly series, let's try a 12-m seasonal difference

diff12 = diff(ts.train, lag=12, differences = 1)['1991-01-01/']
autoplot.zoo(diff12)
par(mfrow=c(1,2))
acf(diff12)
pacf(diff12)
```

## The acf shows a slow decline, and the pacf plot shows a strong drop after lag-1 - both characteristics of an AR-1 process.  The pacf also shows strong correlation at Lags 3, 6, and at 13 (WHAT IS THIS SUPPOSED TO MEAN???)


```{r}
adfPvalue = adf.test(diff12)$p.value
result = "Unit Root Present - Non Stationary Series"
if (adfPvalue < 0.05){ result = " Reject Unit Root Presence - Stationary Series"} 
print(paste(result, " - p-value of adf-test =",adfPvalue))

```


```{r}

#JUST PLAYING WITH OTHER LAGS IN ADDITION TO 12.  SOMEWHAT LOST HERE

diff3 = diff(diff12, lag=3, differences=1)['1991-04-01/']
acf(diff3)
pacf(diff3)
```

